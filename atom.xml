<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>布咯咯_rieuse的博客</title>
  <subtitle>Pyton的世界</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://bulolo.cn/"/>
  <updated>2017-05-26T03:49:19.724Z</updated>
  <id>http://bulolo.cn/</id>
  
  <author>
    <name>布咯咯_rieuse</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python爬虫日记7：批量抓取花瓣网高清美图并保存</title>
    <link href="http://bulolo.cn/2017/05/21/%E7%88%AC%E8%99%AB7/"/>
    <id>http://bulolo.cn/2017/05/21/爬虫7/</id>
    <published>2017-05-21T12:21:35.000Z</published>
    <updated>2017-05-26T03:49:19.724Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：前言"><a href="#一：前言" class="headerlink" title="一：前言"></a>一：前言</h1><p>嘀嘀嘀，上车请刷卡。昨天看到了不错的图片分享网——<a href="http://huaban.com/boards/favorite/beauty/" target="_blank" rel="external">花瓣</a>，里面的图片质量还不错，所以利用selenium+xpath我把它的妹子的栏目下爬取了下来，以图片栏目名称给文件夹命名分类保存到电脑中。这个妹子主页<a href="http://huaban.com/boards/favorite/beauty" target="_blank" rel="external">http://huaban.com/boards/favorite/beauty</a> 是动态加载的，如果想获取更多内容可以模拟下拉，这样就可以更多的图片资源。这种之前爬虫中也做过，但是因为网速不够快所以我就抓了19个栏目，一共500多张美图，也已经很满意了。<br><strong>先看看效果：</strong><br><img src="http://upload-images.jianshu.io/upload_images/4701426-fc6379612e2fe8d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<a id="more"></a>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-ed7262dc8ff7c969.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<h1 id="二：运行环境"><a href="#二：运行环境" class="headerlink" title="二：运行环境"></a>二：运行环境</h1><ul>
<li>IDE：Pycharm</li>
<li>Python3.6</li>
<li>lxml 3.7.2</li>
<li>Selenium 3.4.0</li>
<li>requests 2.12.4</li>
</ul>
<h1 id="三：实例分析"><a href="#三：实例分析" class="headerlink" title="三：实例分析"></a>三：实例分析</h1><p>1.这次爬虫我开始做的思路是：进入这个网页<a href="http://huaban.com/boards/favorite/beauty" target="_blank" rel="external">http://huaban.com/boards/favorite/beauty</a> 然后来获取所有的图片栏目对应网址，然后进入每一个网页中去获取全部图片。（如下图所示）</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-70dc0b6228b7c976.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-f8129fa85d8817fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>2.但是爬取获取的图片分辨率是236x354，图片质量不够高，但是那个时候已经是晚上1点30之后了，所以第二天做了另一个版本：在这个基础上再进入每个缩略图对应的网页，再抓取像下面这样高清的图片。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-8e96a09519b2fa7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<h1 id="四：实战代码"><a href="#四：实战代码" class="headerlink" title="四：实战代码"></a>四：实战代码</h1><p>1.第一步导入本次爬虫需要的模块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">__author__ = &apos;布咯咯_rieuse&apos;</div><div class="line">from selenium.webdriver.common.by import By</div><div class="line">from selenium.webdriver.support import expected_conditions as EC</div><div class="line">from selenium.webdriver.support.ui import WebDriverWait</div><div class="line">from selenium import webdriver</div><div class="line">import requests</div><div class="line">import lxml.html</div><div class="line">import os</div></pre></td></tr></table></figure></p>
<p>2.下面是设置webdriver的种类，就是使用什么浏览器进行模拟，可以使用火狐来看它模拟的过程，也可以是无头浏览器PhantomJS来快速获取资源，[‘–load-images=false’, ‘–disk-cache=true’]这个意思是模拟浏览的时候不加载图片和缓存，这样运行速度会加快一些。WebDriverWait标明最大等待浏览器加载为10秒，set_window_size可以设置一下模拟浏览网页的大小。有些网站如果大小不到位，那么一些资源就不加载出来。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">#  SERVICE_ARGS = [&apos;--load-images=false&apos;, &apos;--disk-cache=true&apos;]</div><div class="line">#  browser = webdriver.PhantomJS(service_args=SERVICE_ARGS)</div><div class="line">browser = webdriver.Firefox()</div><div class="line">wait = WebDriverWait(browser, 10)</div><div class="line">browser.set_window_size(1400, 900)</div></pre></td></tr></table></figure></p>
<p>3.parser(url, param)这个函数用来解析网页，后面有几次都用用到这些代码，所以直接写一个函数会让代码看起来更整洁有序。函数有两个参数：一个是网址，另一个是显性等待代表的部分，这个可以是网页中的某些板块，按钮，图片等等…<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def parser(url, param):</div><div class="line">    browser.get(url)</div><div class="line">    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, param)))</div><div class="line">    html = browser.page_source</div><div class="line">    doc = lxml.html.fromstring(html)</div><div class="line">    return doc</div></pre></td></tr></table></figure></p>
<p>4.下面的代码就是解析本次主页面<a href="http://huaban.com/boards/favorite/beauty/" target="_blank" rel="external">http://huaban.com/boards/favorite/beauty/</a> 然后获取到每个栏目的网址和栏目的名称，使用xpath来获取栏目的网页时，进入网页开发者模式后，如图所示进行操作。之后需要用栏目名称在电脑中建立文件夹，所以在这个网页中要获取到栏目的名称，这里遇到一个问题，一些名称不符合文件命名规则要剔除，我这里就是一个 * 影响了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">def get_main_url():</div><div class="line">    print(&apos;打开主页搜寻链接中...&apos;)</div><div class="line">    try:</div><div class="line">        doc = parser(&apos;http://huaban.com/boards/favorite/beauty/&apos;, &apos;# waterfall&apos;)</div><div class="line">        name = doc.xpath(&apos;//*[@id=&quot;waterfall&quot;]/div/a[1]/div[2]/h3/text()&apos;)</div><div class="line">        u = doc.xpath(&apos;//*[@id=&quot;waterfall&quot;]/div/a[1]/@href&apos;)</div><div class="line">        for item, fileName in zip(u, name):</div><div class="line">            main_url = &apos;http://huaban.com&apos; + item</div><div class="line">            print(&apos;主链接已找到&apos; + main_url)</div><div class="line">            if &apos;*&apos; in fileName:</div><div class="line">                fileName = fileName.replace(&apos;*&apos;, &apos;&apos;)</div><div class="line">            download(main_url, fileName)</div><div class="line">    except Exception as e:</div><div class="line">        print(e)</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-15c45e7520131b7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p>5.前面已经获取到栏目的网页和栏目的名称，这里就需要对栏目的网页分析，进入栏目网页后，只是一些缩略图，我们不想要这些低分辨率的图片，所以要再进入每个缩略图中，解析网页获取到真正的高清图片网址。这里也有一个地方比较坑人，就是一个栏目中，不同的图片存放dom格式不一样，所以我这样做<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">img_url = doc.xpath(&apos;//*[@id=&quot;baidu_image_holder&quot;]/a/img/@src&apos;)</div><div class="line">img_url2 = doc.xpath(&apos;//*[@id=&quot;baidu_image_holder&quot;]/img/@src&apos;)</div></pre></td></tr></table></figure></p>
<p>这就把两种dom格式中的图片地址都获取了，然后把两个地址list合并一下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">img_url +=img_url2</div></pre></td></tr></table></figure>
<p>在本地创建文件夹使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">filename = &apos;image\\&#123;&#125;\\&apos;.format(fileName) + str(i) + &apos;.jpg&apos;</div></pre></td></tr></table></figure>
<p>表示文件保存在与这个爬虫代码同级目录image下，然后获取的图片保存在image中按照之前获取的栏目名称的文件夹中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">def download(main_url, fileName):</div><div class="line">    print(&apos;-------准备下载中-------&apos;)</div><div class="line">    try:</div><div class="line">        doc = parser(main_url, &apos;# waterfall&apos;)</div><div class="line">        if not os.path.exists(&apos;image\\&apos; + fileName):</div><div class="line">            print(&apos;创建文件夹...&apos;)</div><div class="line">            os.makedirs(&apos;image\\&apos; + fileName)</div><div class="line">        link = doc.xpath(&apos;//*[@id=&quot;waterfall&quot;]/div/a/@href&apos;)</div><div class="line">        #  print(link)</div><div class="line">        i = 0</div><div class="line">        for item in link:</div><div class="line">            i += 1</div><div class="line">            minor_url = &apos;http://huaban.com&apos; + item</div><div class="line">            doc = parser(minor_url, &apos;# pin_view_page&apos;)</div><div class="line">            img_url = doc.xpath(&apos;//*[@id=&quot;baidu_image_holder&quot;]/a/img/@src&apos;)</div><div class="line">            img_url2 = doc.xpath(&apos;//*[@id=&quot;baidu_image_holder&quot;]/img/@src&apos;)</div><div class="line">            img_url +=img_url2</div><div class="line">            try:</div><div class="line">                url = &apos;http:&apos; + str(img_url[0])</div><div class="line">                print(&apos;正在下载第&apos; + str(i) + &apos;张图片，地址：&apos; + url)</div><div class="line">                r = requests.get(url)</div><div class="line">                filename = &apos;image\\&#123;&#125;\\&apos;.format(fileName) + str(i) + &apos;.jpg&apos;</div><div class="line">                with open(filename, &apos;wb&apos;) as fo:</div><div class="line">                    fo.write(r.content)</div><div class="line">            except Exception:</div><div class="line">                print(&apos;出错了！&apos;)</div><div class="line">    except Exception:</div><div class="line">        print(&apos;出错啦!&apos;)</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    get_main_url()</div></pre></td></tr></table></figure>
<h1 id="五：总结"><a href="#五：总结" class="headerlink" title="五：总结"></a>五：总结</h1><p>这次爬虫继续练习了Selenium和xpath的使用，在网页分析的时候也遇到很多问题，只有不断练习才能把自己不会部分减少，当然这次爬取了500多张妹纸还是挺养眼的。<br>贴出我的github地址，我的爬虫代码和学习的基础部分都放进去了，有喜欢的朋友一起学习交流吧！<strong><em><a href="https://github.com/rieuse/learnPython" target="_blank" rel="external">github.com/rieuse/learnPython</a></em></strong></p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-a2c3ad6cc56eed72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一：前言&quot;&gt;&lt;a href=&quot;#一：前言&quot; class=&quot;headerlink&quot; title=&quot;一：前言&quot;&gt;&lt;/a&gt;一：前言&lt;/h1&gt;&lt;p&gt;嘀嘀嘀，上车请刷卡。昨天看到了不错的图片分享网——&lt;a href=&quot;http://huaban.com/boards/favorite/beauty/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;花瓣&lt;/a&gt;，里面的图片质量还不错，所以利用selenium+xpath我把它的妹子的栏目下爬取了下来，以图片栏目名称给文件夹命名分类保存到电脑中。这个妹子主页&lt;a href=&quot;http://huaban.com/boards/favorite/beauty&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://huaban.com/boards/favorite/beauty&lt;/a&gt; 是动态加载的，如果想获取更多内容可以模拟下拉，这样就可以更多的图片资源。这种之前爬虫中也做过，但是因为网速不够快所以我就抓了19个栏目，一共500多张美图，也已经很满意了。&lt;br&gt;&lt;strong&gt;先看看效果：&lt;/strong&gt;&lt;br&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/4701426-fc6379612e2fe8d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;Paste_Image.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python爬虫日记6：Selenium+xpath+bs4爬取亚马逊数据保存到mongodb</title>
    <link href="http://bulolo.cn/2017/05/19/%E7%88%AC%E8%99%AB6/"/>
    <id>http://bulolo.cn/2017/05/19/爬虫6/</id>
    <published>2017-05-19T13:21:29.000Z</published>
    <updated>2017-05-23T05:57:51.259Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：前言"><a href="#一：前言" class="headerlink" title="一：前言"></a>一：前言</h1><blockquote>
<p>上周末非常开心，第一次去北京然后参见了zealer和夸克浏览器的联合线下沙龙会议，和大家交流很多收获很多，最让我吃惊的是他们团队非常年轻就有各种能力，每个人都很强。一个结论：我要继续努力！<br>贴上我们的合影，我很帅！：）</p>
</blockquote>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-86fe7ad392d957de.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="zealer&amp;夸克浏览器.jpg"></p>
<a id="more"></a>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-e7a4abb83027103a.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="夸克浏览器合影.JPG"></p>
<blockquote>
<p>这次爬虫是使用selenium来模拟输入关键字（我是测试输入各种图书）然后把全部页数的相关的商品数据保存到mongodb，期间遇到各种问题，很多网站不是很容易就一次可以把网页解析好，很轻松的提取数据。这个亚马逊就是有点怪，这次是提取商品的名称，图片地址，价格，时间，因为我的初始目的是出入有关图书的关键字，所以时间就是图书出版时间。</p>
</blockquote>
<p>关于‘python’关键字如图所示，爬取了300条数据。<br><img src="http://upload-images.jianshu.io/upload_images/4701426-cc1ce81fb481de52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="mongodb数据.png"></p>
<h1 id="二：运行环境"><a href="#二：运行环境" class="headerlink" title="二：运行环境"></a>二：运行环境</h1><ul>
<li>IDE：Pycharm</li>
<li>Python3.6</li>
<li>Selenium 3.4.0</li>
<li>pymongo 3.3.0</li>
<li>BeautifulSoup 4.5.3</li>
</ul>
<h1 id="三：-爬虫中重要（keng）的部分"><a href="#三：-爬虫中重要（keng）的部分" class="headerlink" title="三： 爬虫中重要（keng）的部分"></a>三： 爬虫中重要（keng）的部分</h1><ul>
<li>商品的时间使用Beautifulsoup是提取不出来的，使用正则表达式也搞不定，我最后用xpath才提取出来</li>
<li>每个商品框架都是独立id，没有使用共同的class，所以要想获取他们使用正则表达式挺合适的</li>
<li>因为商品的名称，图片地址，价格这三个是使用beautifulsoup提取的，而时间是用的xpath提取，要想把他们一起装入一个字典中然后写入mongodb就需要用到zip这个函数了。<br>像这样的处理两个列表一起迭代<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">for item, time in zip(content, date)</div></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="四：实战代码"><a href="#四：实战代码" class="headerlink" title="四：实战代码"></a>四：实战代码</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line">from selenium.common.exceptions import TimeoutException</div><div class="line">from selenium.webdriver.common.by import By</div><div class="line">from selenium.webdriver.support import expected_conditions as EC</div><div class="line">from selenium.webdriver.support.ui import WebDriverWait</div><div class="line">from selenium import webdriver</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import lxml.html</div><div class="line">import pymongo</div><div class="line">import re</div><div class="line"></div><div class="line">MONGO_URL = &apos;localhost&apos;</div><div class="line">MONGO_DB = &apos;amazon&apos;</div><div class="line">MONGO_TABLE = &apos;amazon-python&apos;</div><div class="line">SERVICE_ARGS = [&apos;--load-images=false&apos;, &apos;--disk-cache=true&apos;]</div><div class="line">KEYWORD = &apos;python&apos;</div><div class="line">client = pymongo.MongoClient(MONGO_URL)</div><div class="line">db = client[MONGO_DB]</div><div class="line"></div><div class="line">browser = webdriver.PhantomJS(service_args=SERVICE_ARGS)</div><div class="line">#  browser = webdriver.Firefox()</div><div class="line">wait = WebDriverWait(browser, 10)</div><div class="line">browser.set_window_size(1400, 900)</div><div class="line"></div><div class="line"></div><div class="line">def search():</div><div class="line">    print(&apos;正在搜索&apos;)</div><div class="line">    try:</div><div class="line">        browser.get(&apos;https://www.amazon.cn/&apos;)</div><div class="line">        input = wait.until(</div><div class="line">            EC.presence_of_element_located((By.CSS_SELECTOR, &apos;# twotabsearchtextbox&apos;))</div><div class="line">        )</div><div class="line">        submit = wait.until(</div><div class="line">            EC.element_to_be_clickable((By.CSS_SELECTOR, &apos;# nav-search &gt; form &gt; div.nav-right &gt; div &gt; input&apos;)))</div><div class="line">        input.send_keys(KEYWORD)</div><div class="line">        submit.click()</div><div class="line">        total = wait.until(</div><div class="line">            EC.presence_of_element_located((By.CSS_SELECTOR, &apos;# pagn &gt; span.pagnDisabled&apos;)))</div><div class="line">        get_products()</div><div class="line">        print(&apos;一共&apos; + total.text + &apos;页&apos;)</div><div class="line">        return total.text</div><div class="line">    except TimeoutException:</div><div class="line">        return search()</div><div class="line"></div><div class="line"></div><div class="line">def next_page(number):</div><div class="line">    print(&apos;正在翻页&apos;, number)</div><div class="line">    try:</div><div class="line">        wait.until(EC.text_to_be_present_in_element(</div><div class="line">            (By.CSS_SELECTOR, &apos;# pagnNextString&apos;), &apos;下一页&apos;))</div><div class="line">        submit = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, &apos;# pagnNextString&apos;)))</div><div class="line">        submit.click()</div><div class="line">        wait.until(EC.text_to_be_present_in_element(</div><div class="line">            (By.CSS_SELECTOR, &apos;.pagnCur&apos;), str(number)))</div><div class="line">        get_products()</div><div class="line">    except TimeoutException:</div><div class="line">        next_page(number)</div><div class="line"></div><div class="line"></div><div class="line">def get_products():</div><div class="line">    try:</div><div class="line">        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, &apos;# s-results-list-atf&apos;)))</div><div class="line">        html = browser.page_source</div><div class="line">        soup = BeautifulSoup(html, &apos;lxml&apos;)</div><div class="line">        doc = lxml.html.fromstring(html)</div><div class="line">        date = doc.xpath(&apos;//*[@class=&quot;s-result-item  celwidget &quot;]/div/div[2]/div[1]/span[2]/text()&apos;)</div><div class="line">        content = soup.find_all(attrs=&#123;&quot;id&quot;: re.compile(r&apos;result_\d+&apos;)&#125;)</div><div class="line">        for item, time in zip(content, date):</div><div class="line">            product = &#123;</div><div class="line">                &apos;title&apos;: item.find(class_=&apos;s-access-title&apos;).get_text(),</div><div class="line">                &apos;image&apos;: item.find(class_=&apos;s-access-image cfMarker&apos;).get(&apos;src&apos;),</div><div class="line">                &apos;price&apos;: item.find(class_=&apos;a-size-base a-color-price s-price a-text-bold&apos;).get_text(),</div><div class="line">                &apos;date&apos;: time</div><div class="line">            &#125;</div><div class="line">            save_to_mongo(product)</div><div class="line">            print(product)</div><div class="line">    except Exception as e:</div><div class="line">        print(e)</div><div class="line"></div><div class="line"></div><div class="line">def save_to_mongo(result):</div><div class="line">    try:</div><div class="line">        if db[MONGO_TABLE].insert(result):</div><div class="line">            print(&apos;存储到mongodb成功&apos;, result)</div><div class="line">    except Exception:</div><div class="line">        print(&apos;存储到mongodb失败&apos;, result)</div><div class="line"></div><div class="line"></div><div class="line">def main():</div><div class="line">    try:</div><div class="line">        total = search()</div><div class="line">        total = int(re.compile(&apos;(\d+)&apos;).search(total).group(1))</div><div class="line">        for i in range(2, total + 1):</div><div class="line">            next_page(i)</div><div class="line">    except Exception as e:</div><div class="line">        print(&apos;出错啦&apos;, e)</div><div class="line">    finally:</div><div class="line">        browser.close()</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    main()</div></pre></td></tr></table></figure>
<h1 id="五：总结"><a href="#五：总结" class="headerlink" title="五：总结"></a>五：总结</h1><p>这次学习的东西还是很多，selenium用的模块很多，也利用了无头浏览器PhantomJS的不加载图片和缓存。爬取数据的时候使用了不同的方式，并用zip函数一起迭代保存为字典成功导入到mongodb中。<br>贴出我的github地址，我的爬虫代码和学习的基础部分都放进去了，有喜欢的朋友一起学习交流吧！<strong><em><a href="https://github.com/rieuse/learnPython" target="_blank" rel="external">github.com/rieuse/learnPython</a></em></strong></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一：前言&quot;&gt;&lt;a href=&quot;#一：前言&quot; class=&quot;headerlink&quot; title=&quot;一：前言&quot;&gt;&lt;/a&gt;一：前言&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;上周末非常开心，第一次去北京然后参见了zealer和夸克浏览器的联合线下沙龙会议，和大家交流很多收获很多，最让我吃惊的是他们团队非常年轻就有各种能力，每个人都很强。一个结论：我要继续努力！&lt;br&gt;贴上我们的合影，我很帅！：）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/4701426-86fe7ad392d957de.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;zealer&amp;amp;夸克浏览器.jpg&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python实现斐波那契数列方法及其优化总结</title>
    <link href="http://bulolo.cn/2017/05/15/python1/"/>
    <id>http://bulolo.cn/2017/05/15/python1/</id>
    <published>2017-05-15T05:04:42.000Z</published>
    <updated>2017-05-23T05:08:31.902Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>斐波那契数列的相关题目是面试常见的，所以我看了些资料总结记录一下这些小的知识点。</p>
</blockquote>
<h3 id="1-元组实现"><a href="#1-元组实现" class="headerlink" title="1. 元组实现"></a>1. 元组实现</h3><p>代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">fibs = [0, 1]</div><div class="line">for i in range(8):</div><div class="line">    fibs.append(fibs[-2] + fibs[-1])</div><div class="line">print(fibs)</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]</div></pre></td></tr></table></figure></p>
<h3 id="2-迭代器实现"><a href="#2-迭代器实现" class="headerlink" title="2. 迭代器实现"></a>2. 迭代器实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">class Fibs:</div><div class="line">    def __init__(self):</div><div class="line">        self.a = 0</div><div class="line">        self.b = 1</div><div class="line"></div><div class="line">    def next(self):</div><div class="line">        self.a, self.b = self.b, self.a + self.b</div><div class="line">        return self.a</div><div class="line"></div><div class="line">    def __iter__(self):</div><div class="line">        return self</div></pre></td></tr></table></figure>
<p>这将得到一个无穷的数列， 可以采用如下方式访问：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">fibs = Fibs()</div><div class="line">for f in fibs:</div><div class="line">    if f &gt; 1000:</div><div class="line">        print(f)</div><div class="line">        break</div><div class="line">    else:</div><div class="line">        print(f)</div></pre></td></tr></table></figure></p>
<h3 id="3-通过定制类实现"><a href="#3-通过定制类实现" class="headerlink" title="3. 通过定制类实现"></a>3. 通过定制类实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">class Fib(object):</div><div class="line">    def __getitem__(self, n):</div><div class="line">        if isinstance(n, int):</div><div class="line">            a, b = 1, 1</div><div class="line">            for x in range(n):</div><div class="line">                a, b = b, a + b</div><div class="line">            return a</div><div class="line">        elif isinstance(n, slice):</div><div class="line">            start = n.start</div><div class="line">            stop = n.stop</div><div class="line">            a, b = 1, 1</div><div class="line">            L = []</div><div class="line">            for x in range(stop):</div><div class="line">                if x &gt;= start:</div><div class="line">                    L.append(a)</div><div class="line">                a, b = b, a + b</div><div class="line">            return L</div><div class="line">        else:</div><div class="line">            raise TypeError(&quot;Fib indices must be integers&quot;)</div></pre></td></tr></table></figure>
<p>这样可以得到一个类似于序列的数据结构，可以通过下标来访问数据：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">f = Fib()</div><div class="line">print (f[0:10])</div></pre></td></tr></table></figure></p>
<h3 id="4-Python实现比较简易的斐波那契数列示例"><a href="#4-Python实现比较简易的斐波那契数列示例" class="headerlink" title="4.Python实现比较简易的斐波那契数列示例"></a>4.Python实现比较简易的斐波那契数列示例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">i, j = 0, 1</div><div class="line">while i &lt; 10000:</div><div class="line"> print( i,j, = j, i+j)</div></pre></td></tr></table></figure>
<p> 最后展示运行结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0 1 1 2 3 5 8 13 21 34 55 89 144 233 377 610 987 1597 2584 4181 6765</div></pre></td></tr></table></figure></p>
<h3 id="5-列表生成式实现"><a href="#5-列表生成式实现" class="headerlink" title="5.列表生成式实现"></a>5.列表生成式实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def fib(n):</div><div class="line">    if n == 1 or n == 0:</div><div class="line">        return 1</div><div class="line">    else:</div><div class="line">        return fib(n - 2) + fib(n - 1)</div><div class="line">print([fib(n) for n in range(10)])</div></pre></td></tr></table></figure>
<p>这个计算斐波那契数列前n项很简单，但是从下面的图可以看出这个计算花费的时间较多因为会重复计算很多值。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-be5adfbba96c60a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>这个时候我需要修改一下，加入<strong>缓存</strong>机制。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">def fib(n, cache=None):</div><div class="line">    if cache is None:</div><div class="line">        cache = &#123;&#125;</div><div class="line">    if n in cache:</div><div class="line">        return cache[n]</div><div class="line">    if n == 1 or n == 0:</div><div class="line">        return 1</div><div class="line">    else:</div><div class="line">        cache[n] = fib(n - 2, cache) + fib(n - 1, cache)</div><div class="line">        return cache[n]</div><div class="line">print([fib(n) for n in range(999)])</div></pre></td></tr></table></figure></p>
<p>这样即使是n的值很大也能很快的计算很出来。</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;斐波那契数列的相关题目是面试常见的，所以我看了些资料总结记录一下这些小的知识点。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-元组实现&quot;&gt;&lt;a href=&quot;#1-元组实现&quot; class=&quot;headerlink&quot; title=&quot;1. 元组实现&quot;&gt;&lt;/a&gt;1. 元组实现&lt;/h3&gt;&lt;p&gt;代码：&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;fibs = [0, 1]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;for i in range(8):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    fibs.append(fibs[-2] + fibs[-1])&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;print(fibs)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python爬虫日记5：使用Selenium爬取一点资讯动态数据</title>
    <link href="http://bulolo.cn/2017/05/05/%E7%88%AC%E8%99%AB5/"/>
    <id>http://bulolo.cn/2017/05/05/爬虫5/</id>
    <published>2017-05-05T13:21:22.000Z</published>
    <updated>2017-05-23T10:48:35.605Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：前言"><a href="#一：前言" class="headerlink" title="一：前言"></a>一：前言</h1><p>这几天哈尔滨天气天天刮风下雨的挺烦的，抱个电脑去学校图书馆学编程回来还要被雨淋，马上专业课也要考试了，Python集中学习要等到几门课考完了。<br>今天使用Selenium来处理JS一点资讯文章动态加载问题，本来是想配合PhantomJS无界面浏览器来实现的，但是一直出问题等有空在找找原因吧，所以我就Firefox()了。</p>
<blockquote>
<p><strong>目标：获取一点资讯动态文章信息并以csv格式保存</strong></p>
</blockquote>
<a id="more"></a>
<h1 id="二：运行环境"><a href="#二：运行环境" class="headerlink" title="二：运行环境"></a>二：运行环境</h1><ul>
<li><p>Python3.6，Anaconda集成版本，方便管理各种模块。</p>
</li>
<li><p>Selenium 3.4.0</p>
</li>
</ul>
<h1 id="三：实例分析"><a href="#三：实例分析" class="headerlink" title="三：实例分析"></a>三：实例分析</h1><p>1.先看看网站<a href="http://www.yidianzixun.com/channel/c6" target="_blank" rel="external">一点资讯</a>，的分析，红色部分是文章标题，文章作者，还有评价数目，这几个是我需要提取的数据，右边的按钮是用来刷新新文章的一会儿要用到。<br><img src="http://upload-images.jianshu.io/upload_images/4701426-06a97c18ccf6ca8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="一点资讯1.png"><br>2.进入开发者模式后找到相应位置可以看到文章链接，标题，文章作者，评论数目。<br><img src="http://upload-images.jianshu.io/upload_images/4701426-888421d366bdf520.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="一点资讯2.png"><br>3.但是从下面看到首页的这样的新闻可以爬取的只有几个而已，我们如果想爬取多一点怎么办呢？当我们打开这个页面的时候，鼠标滚轮向下滚动的时候发现这些数据就变多了，说明这是一个JS动态加载数据的方式。我这次就要用到Selenium来模拟浏览器从而获取更多文章信息。<br><img src="http://upload-images.jianshu.io/upload_images/4701426-fb628f5d5da9f800.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="一点资讯3.png"></p>
<h1 id="四：实战代码"><a href="#四：实战代码" class="headerlink" title="四：实战代码"></a>四：实战代码</h1><p>1.首先把全部的模块导入一下，这次用到了好几个。selenium.webdriver用来模拟浏览器用到的；BeautifulSoup用来解析网页结构；csv模块用来把数据保存为csv格式；time用来延时的，不然网页没有加载完就解析数据，那么保存的数据不完整，不够多。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">from selenium.webdriver.common.keys import Keys</div><div class="line">from selenium import webdriver</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import csv,time</div></pre></td></tr></table></figure></p>
<p>2.首先webdriver.Firefox()模拟一个火狐浏览器，之后请求这个一点资讯地址延时2秒，保证加载完成。这里本打算用PhantomJS无界面浏览器来爬取的，但是结果不够好，暂时就用火狐吧。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">driver = webdriver.Firefox()</div><div class="line">first_url = &apos;http://www.yidianzixun.com/channel/c6&apos;</div><div class="line">driver.get(first_url)</div><div class="line">time.sleep(2)</div></pre></td></tr></table></figure></p>
<p>3.接下来模拟鼠标点击那个刷新按钮可以加载更多数据，次数可以自己在加，我这里就用一次，icon-refresh是在开发者模式中按照我图中步骤找到那个按钮的相应位置，使用快捷键Ctrl+Shift+c，然后点击那个刷新按钮就可以跳转到相应代码位置。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-ea432b133fa4bd58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>4.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">find_element_by_class_name().click()</div></pre></td></tr></table></figure></p>
<p>来模拟点击按钮，之前导入的模块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">from selenium.webdriver.common.keys import Keys</div></pre></td></tr></table></figure></p>
<p>这个是模拟键盘的操作的模块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">driver.find_element_by_class_name(&apos;icon-refresh&apos;).send_keys(Keys.DOWN)</div></pre></td></tr></table></figure></p>
<p>用来模拟键盘的<strong> ↓ </strong>方向键，这样就可以使侧边的滚动条往下滚动，从而实现动态加载文章数据。然后也需要延迟一下让网页加载完全。这里的for语句执行多次是为了按下↓键更多。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">driver.find_element_by_class_name(&apos;icon-refresh&apos;).click()</div><div class="line">for i in range(1,90):</div><div class="line">    driver.find_element_by_class_name(&apos;icon-refresh&apos;).send_keys(Keys.DOWN)</div><div class="line">time.sleep(3)</div></pre></td></tr></table></figure></p>
<p>5.到现在网页已经加载完全了，我们想要的文章数据也加载很多了，那么就开始解析网页来爬取数据吧！通过上面实例分析的第二步可以知道文章标题，作者，评论数目，文章链接等信息位置，最后解析完就把这个模拟的浏览器关闭。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">soup = BeautifulSoup(driver.page_source, &apos;lxml&apos;)</div><div class="line">articles = []</div><div class="line">for article in soup.find_all(class_=&apos;item doc style-small-image style-content-middle&apos;):</div><div class="line">    title = article.find(class_=&apos;doc-title&apos;).get_text()</div><div class="line">    source = article.find(class_=&apos;source&apos;).get_text()</div><div class="line">    comment = article.find(class_=&apos;comment-count&apos;).get_text()</div><div class="line">    link = &apos;http://www.yidianzixun.com&apos; + article.get(&apos;href&apos;)</div><div class="line">    articles.append([title, source, comment, link])</div><div class="line">driver.quit()</div></pre></td></tr></table></figure></p>
<p>6.数据也获取了那么我们现在使用csv格式来保存这些数据就行了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">with open(&apos;yidian.csv&apos;, &apos;w&apos;) as f:</div><div class="line">    writer = csv.writer(f)</div><div class="line">    writer.writerow([&apos;文章标题&apos;, &apos;作者&apos;, &apos;评论数&apos;, &apos;文章地址&apos;])</div><div class="line">    for row in articles:</div><div class="line">        writer.writerow(row)</div></pre></td></tr></table></figure></p>
<h1 id="五：总结"><a href="#五：总结" class="headerlink" title="五：总结"></a>五：总结</h1><p>这次爬虫练习了selenium模拟浏览器的各种操作，继续加油！<br>贴出我的github地址，我的爬虫代码和学习的基础部分都放进去了，有喜欢的朋友一起学习交流吧！<strong><em><a href="https://github.com/rieuse/learnPython" target="_blank" rel="external">github.com/rieuse/learnPython</a></em></strong></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一：前言&quot;&gt;&lt;a href=&quot;#一：前言&quot; class=&quot;headerlink&quot; title=&quot;一：前言&quot;&gt;&lt;/a&gt;一：前言&lt;/h1&gt;&lt;p&gt;这几天哈尔滨天气天天刮风下雨的挺烦的，抱个电脑去学校图书馆学编程回来还要被雨淋，马上专业课也要考试了，Python集中学习要等到几门课考完了。&lt;br&gt;今天使用Selenium来处理JS一点资讯文章动态加载问题，本来是想配合PhantomJS无界面浏览器来实现的，但是一直出问题等有空在找找原因吧，所以我就Firefox()了。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;目标：获取一点资讯动态文章信息并以csv格式保存&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python爬虫日记4：Charles抓包获取黑大帐号密码验证码并登录</title>
    <link href="http://bulolo.cn/2017/05/02/%E7%88%AC%E8%99%AB4/"/>
    <id>http://bulolo.cn/2017/05/02/爬虫4/</id>
    <published>2017-05-02T11:44:16.000Z</published>
    <updated>2017-05-23T05:05:12.632Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：前言"><a href="#一：前言" class="headerlink" title="一：前言"></a>一：前言</h1><p>今天看了一篇安利Charles这个软件的文章，就拿来试试，我们大学的登录页面用开发者模式进去chrome有屏蔽相关模块，用火狐可以正常不过还是抓不到验证码这个js动态数据而且帐号密码的请求后Cookies并找不到。那么这个时候使用抓包软件就是一个好的方法之一了，之前也用过其他抓包软件，比如Fidder，今天用过Charles后才发现还有比Fidder好用的抓包软件，这个比较简洁，数据查找也很直观。</p>
<blockquote>
<p><strong>目标：</strong>使用抓包软件Charles对页面数据分析找到帐号密码以及验证码的接口，然后用Python实现模拟登录，并提取登录后的页面。</p>
</blockquote>
<a id="more"></a>
<h1 id="二：运行环境"><a href="#二：运行环境" class="headerlink" title="二：运行环境"></a>二：运行环境</h1><ul>
<li><p>Python3.6，我用的是Anaconda集成版本，方便管理各种模块。</p>
</li>
<li><p>Charles版本是4.02，使用很简单，数据显示直观。</p>
</li>
</ul>
<h1 id="三：实例分析"><a href="#三：实例分析" class="headerlink" title="三：实例分析"></a>三：实例分析</h1><p>1.分析网站登录情况，网址是<a href="http://my.hlju.edu.cn/login.portal" target="_blank" rel="external">http://my.hlju.edu.cn/login.portal</a> 进去之后用火狐的浏览器进去开发者模式，看到了验证码地址captchaGenerate.portal?后面跟的随机数字代表的不同的验证码，我把这个配合主网址组成这个网址 <a href="http://my.hlju.edu.cn/captchaGenerate.portal?" target="_blank" rel="external">http://my.hlju.edu.cn/captchaGenerate.portal?</a> 在浏览器打开就是随机的验证码。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-c69e3728e1ed797f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-b99d0c5a741ef9ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>2.验证码的网址已经找到了，现在我们使用Charles抓包工具，抓取登录时的数据分析一下，这一张是抓包后的图。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-39b7a8e18720e86b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>3.然后点击这个userPasswordValidate.portal，可知道这个保存着登录的全部数据，我们点击一下From数据就变得整洁多了，可以看到有几个键值对这样我们帐号密码对应地址也找到了，之后就可以开始用Python模拟登录了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Login.Token1	*******</div><div class="line">Login.Token2	*******</div><div class="line">captcha	w4dy</div><div class="line">goto	http://my.hlju.edu.cn/loginSuccess.portal</div><div class="line">gotoOnFail	http://my.hlju.edu.cn/loginFailure.portal</div></pre></td></tr></table></figure></p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-7d84cb900c2e9388.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"></p>
<h1 id="四：实战代码"><a href="#四：实战代码" class="headerlink" title="四：实战代码"></a>四：实战代码</h1><p>帐号密码改成自己的学号密码即可模拟登录，之前爬虫都没有使用requests.session()，这里就需要因为用了这个回话对象，可以使几次请求都在同一个Cookie下进行，方便我们模拟登录后获取登录后的主页面。<br>会话对象让你能够跨请求保持某些参数。它也会在同一个 Session 实例发出的所有请求之间保持 cookie， 期间使用 urllib3 <a href="https://urllib3.readthedocs.io/en/latest/pools.html" target="_blank" rel="external">connection pooling</a> 功能。所以如果你向同一主机发送多个请求，底层的 TCP 连接将会被重用，从而带来显著的性能提升。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line">from PIL import Image</div><div class="line">from bs4 import BeautifulSoup</div><div class="line"></div><div class="line">url1 = &apos;http://my.hlju.edu.cn/captchaGenerate.portal?&apos;</div><div class="line">url2 = &apos;http://my.hlju.edu.cn/userPasswordValidate.portal&apos;</div><div class="line">url3 = &apos;http://my.hlju.edu.cn&apos;</div><div class="line">headers = &#123;</div><div class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36&apos;</div><div class="line">&#125;</div><div class="line">s = requests.session()</div><div class="line">response = s.get(url1, headers=headers)</div><div class="line">html = response.text</div><div class="line">soup = BeautifulSoup(html, &apos;html.parser&apos;)</div><div class="line">with open(&apos;img\code.jpg&apos;, &apos;wb&apos;) as f:</div><div class="line">    f.write(response.content)</div><div class="line">img = Image.open(&apos;img\code.jpg&apos;)</div><div class="line">img.show()</div><div class="line">data = &#123;&#125;</div><div class="line">data[&apos;Login.Token1&apos;] = &apos;帐号&apos;</div><div class="line">data[&apos;Login.Token2&apos;] = &apos;密码&apos;</div><div class="line">data[&apos;captcha&apos;] = input(&apos;输入验证码：&apos;)</div><div class="line">data[&apos;goto&apos;] = &apos;http://my.hlju.edu.cn/loginSuccess.portal&apos;</div><div class="line">data[&apos;gotoOnFail&apos;] = &apos;http://my.hlju.edu.cn/loginFailure.portal&apos;</div><div class="line">response2 = s.post(url=url2, data=data, headers=headers)</div><div class="line">response3 = s.get(url3, headers=headers)</div><div class="line">print(response3.text)</div></pre></td></tr></table></figure></p>
<h1 id="五：总结"><a href="#五：总结" class="headerlink" title="五：总结"></a>五：总结</h1><p>这次练习了一下Charles抓包的使用和对抓包数据的分析，每天写一写小Demo，继续加油！<br>这里贴出我的github地址，我的爬虫代码和学习的基础部分都放进去了，有喜欢的朋友一起学习交流吧！<a href="https://github.com/rieuse/learnPython" target="_blank" rel="external">github.com/rieuse/learnPython</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一：前言&quot;&gt;&lt;a href=&quot;#一：前言&quot; class=&quot;headerlink&quot; title=&quot;一：前言&quot;&gt;&lt;/a&gt;一：前言&lt;/h1&gt;&lt;p&gt;今天看了一篇安利Charles这个软件的文章，就拿来试试，我们大学的登录页面用开发者模式进去chrome有屏蔽相关模块，用火狐可以正常不过还是抓不到验证码这个js动态数据而且帐号密码的请求后Cookies并找不到。那么这个时候使用抓包软件就是一个好的方法之一了，之前也用过其他抓包软件，比如Fidder，今天用过Charles后才发现还有比Fidder好用的抓包软件，这个比较简洁，数据查找也很直观。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;目标：&lt;/strong&gt;使用抓包软件Charles对页面数据分析找到帐号密码以及验证码的接口，然后用Python实现模拟登录，并提取登录后的页面。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python爬虫日记3：爬取v2ex数据用csv保存</title>
    <link href="http://bulolo.cn/2017/05/02/%E7%88%AC%E8%99%AB3/"/>
    <id>http://bulolo.cn/2017/05/02/爬虫3/</id>
    <published>2017-05-02T04:44:11.000Z</published>
    <updated>2017-05-23T05:00:16.543Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：前言"><a href="#一：前言" class="headerlink" title="一：前言"></a>一：前言</h1><hr>
<p>v2ex是一个汇集各类奇妙好玩的话题和流行动向的网站，有很多不错的问答。这次爬虫是五一期间做的，贴出来网址<a href="https://www.v2ex.com/?tab=all。" target="_blank" rel="external">https://www.v2ex.com/?tab=all。</a></p>
<blockquote>
<p><strong>目标：</strong>爬取全部分类中的文章标题，分类，作者，文章地址这些内容然后以csv格式保存下来。</p>
</blockquote>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-9ae03cf9f5b89721.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br><a id="more"></a></p>
<h2 id="二：说明"><a href="#二：说明" class="headerlink" title="#二：说明"></a>#二：说明</h2><ul>
<li>本次使用的是Python3.6版本</li>
<li>作者这个内容是js动态数据  使用xpath Beautifulsoup的tag和select都抓取不到，我试了试用正则表达式可以，目前还没学其他方法就这样头铁了。</li>
<li>使用csv保存数据的时候我发现writer.writerow()和writer.writerows()是不一样的，本次用的前者。</li>
</ul>
<h1 id="三：实战分析"><a href="#三：实战分析" class="headerlink" title="三：实战分析"></a>三：实战分析</h1><hr>
<p>1.导入本次使用的模块，csv， re， requests， BeautifulSoup。<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import csv, requests, re</div><div class="line">from bs4 import BeautifulSoup</div></pre></td></tr></table></figure></p>
<p>2.请求网页与解析网页。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">url = &apos;https://www.v2ex.com/?tab=all&apos;</div><div class="line">html = requests.get(url).text</div><div class="line">soup = BeautifulSoup(html, &apos;html.parser&apos;)</div></pre></td></tr></table></figure></p>
<p>3.先看一下网页结构。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-128de4d1b5ff7cd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>然后来获取文章标题，分类，作者，文章地址，这里的标题和分类都很容易获取，使用BeautifulSoup解析后按照class就可以找到，然后使用get_text()即可获取我们需要的内容，最头疼的是作者和文章链接，我这里使用正则才把他们挖掘出来，不过也算是练习正则表达式的使用。最后把获取的内容都传给articles列表。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">articles = []</div><div class="line">for article in soup.find_all(class_=&apos;cell item&apos;):</div><div class="line">    title = article.find(class_=&apos;item_title&apos;).get_text()</div><div class="line">    category = article.find(class_=&apos;node&apos;).get_text()</div><div class="line">    author = re.findall(r&apos;(?&lt;=&lt;a href=&quot;/member/).+(?=&quot;&gt;&lt;img)&apos;, str(article))[0]</div><div class="line">    u = article.select(&apos;.item_title &gt; a&apos;)</div><div class="line">    link = &apos;https://www.v2ex.com&apos; + re.findall(r&apos;(?&lt;=href=&quot;).+(?=&quot;)&apos;, str(u))[0]</div><div class="line">    articles.append([title, category, author, link])</div></pre></td></tr></table></figure></p>
<p>4.把列表中的数据保存在csv中，并且给他们第一行写入标题。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">with open(&apos;v2ex.csv&apos;, &apos;w&apos;) as f:</div><div class="line">    writer = csv.writer(f)</div><div class="line">    writer.writerow([&apos;文章标题&apos;, &apos;分类&apos;, &apos;作者&apos;, &apos;文章地址&apos;])</div><div class="line">    for row in articles:</div><div class="line">        writer.writerow(row)</div></pre></td></tr></table></figure></p>
<h1 id="四：总结"><a href="#四：总结" class="headerlink" title="四：总结"></a>四：总结</h1><hr>
<p>最后的效果：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-62d7f0746576de00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>这次爬取遇到了一些问题，慢慢的学会更多东西，爬虫让我非常快乐。我以后会坚持写下去，有喜欢的朋友一起学习交流吧！<br>这里贴出我的github地址，我的爬虫代码和学习的基础部分都放进去了。<br><a href="https://github.com/rieuse/learnPython" target="_blank" rel="external">https://github.com/rieuse/learnPython</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一：前言&quot;&gt;&lt;a href=&quot;#一：前言&quot; class=&quot;headerlink&quot; title=&quot;一：前言&quot;&gt;&lt;/a&gt;一：前言&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;v2ex是一个汇集各类奇妙好玩的话题和流行动向的网站，有很多不错的问答。这次爬虫是五一期间做的，贴出来网址&lt;a href=&quot;https://www.v2ex.com/?tab=all。&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.v2ex.com/?tab=all。&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;目标：&lt;/strong&gt;爬取全部分类中的文章标题，分类，作者，文章地址这些内容然后以csv格式保存下来。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/4701426-9ae03cf9f5b89721.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;Paste_Image.png&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python爬虫日记2：使用lxml解析HTML输出对应值</title>
    <link href="http://bulolo.cn/2017/04/28/%E7%88%AC%E8%99%AB2/"/>
    <id>http://bulolo.cn/2017/04/28/爬虫2/</id>
    <published>2017-04-28T11:07:05.000Z</published>
    <updated>2017-05-23T04:59:32.425Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h1><p>今天我要做的是爬取凤凰网资讯的一个即时新闻列表的标题和对应链接，很简单的requests与lxml练习，同时使用xpath。贴出网址：<a href="http://news.ifeng.com/listpage/11502/0/1/rtlist.shtml" target="_blank" rel="external">http://news.ifeng.com/listpage/11502/0/1/rtlist.shtml</a></p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-7aaff42d387ccea6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="凤凰资讯.png"><br><a id="more"></a></p>
<h1 id="二、运行环境"><a href="#二、运行环境" class="headerlink" title="二、运行环境"></a>二、运行环境</h1><ul>
<li>系统版本<br>Windows10  64位</li>
<li>Python版本<br>Python3.6  我用的是Anaconda集成版本</li>
<li>IDE<br>PyCharm 学生可以通过edu邮箱免费使用，不是学生的朋友可以试试社区版。</li>
</ul>
<h1 id="三、分析"><a href="#三、分析" class="headerlink" title="三、分析"></a>三、分析</h1><p>解析HTML常用方式有<strong>BeautifulSoup</strong>,<strong>lxml.html</strong>,性能方面lxml要优于BeautifulSoup，BeautifulSoup是基于DOM的，会解析整个DOM树，lxml只会局部遍历。</p>
<p>python3网络请求常用的有自带的urllib，第三方库requests，使用起来requests还是比urllib更简单明了，而且requests有更强的功能。</p>
<h1 id="四、实战"><a href="#四、实战" class="headerlink" title="四、实战"></a>四、实战</h1><p>首先导入今天需要的模块requests，lxml.html。</p>
<pre><code>import requests
import lxml.html
</code></pre><p>然后url是目标网址，html保存着这个网页的文本内容，这时候需用lxml来解析它，这样才能提取我们需要的数据。</p>
<pre><code>url = &apos;http://news.ifeng.com/listpage/11502/0/1/rtlist.shtml&apos;
html = requests.get(url).text
doc = lxml.html.fromstring(html)
</code></pre><p>解析完成后，我们首先提取文章的标题，这里使用了xpath来搜索标题所在的标签，对原网址F12 开发者模式打开可以查询标题。<br><img src="http://upload-images.jianshu.io/upload_images/4701426-8f61ab5ce62f6e3d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="凤凰资讯标题.png"></p>
<pre><code>titles = doc.xpath(&apos;//div[@class=&quot;newsList&quot;]/ul/li/a/text()&apos;)
href = doc.xpath(&apos;//div[@class=&quot;newsList&quot;]/ul/li/a/@href&apos;)
</code></pre><p>这里第一行是将网页中的符合标题的内容都传给titles变量中，第二行是将标题所在的网址全部传给href。</p>
<p>说到这个xpath查询有很多人不太会用，或者觉得很麻烦，不过这里推荐一款xpath查询插件，这样我们查询目标的时候就很容易获取了。这款chrome插件是xpath heper ，安装好之后我们重新打开浏览器按ctrl+shift+x就能调出xpath-helper框了，按shift配合鼠标可以切换查询的目标。</p>
<p>最后一步：将标题和对应的网址结合起来，遍历后输出即可看到结果<br>    i = 0<br>    for content in titles:<br>        results = {<br>            ‘标题’:titles[i],<br>            ‘链接’:href[i]<br>        }<br>        i += 1<br>        print(results)</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-6172297303818caf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="凤凰资讯2.png"></p>
<h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h1><p>查询标签用BeautifulSoup也挺合适的，这次为了练习一下就使用了lxml 配合xpath。继续努力，给自己加油！ヾ(o◕∀◕)ﾉヾ</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h1&gt;&lt;p&gt;今天我要做的是爬取凤凰网资讯的一个即时新闻列表的标题和对应链接，很简单的requests与lxml练习，同时使用xpath。贴出网址：&lt;a href=&quot;http://news.ifeng.com/listpage/11502/0/1/rtlist.shtml&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://news.ifeng.com/listpage/11502/0/1/rtlist.shtml&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/4701426-7aaff42d387ccea6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;凤凰资讯.png&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python爬虫日记1：爬取豆瓣电影中速度与激情8演员图片</title>
    <link href="http://bulolo.cn/2017/04/27/%E7%88%AC%E8%99%AB1/"/>
    <id>http://bulolo.cn/2017/04/27/爬虫1/</id>
    <published>2017-04-27T08:34:19.000Z</published>
    <updated>2017-05-23T10:52:50.437Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h1><p>这是我第一次写文章，作为一个非计算机，编程类专业的大二学生，我希望能够给像我这样的入门的朋友一些帮助，也同时激励自己努力写代码。好了废话不多说，今天我做的爬虫是豆瓣的一个电影——速度与激情8的全部影人页面，贴出网址：<a href="https://movie.douban.com/subject/26260853/celebrities" target="_blank" rel="external">速度与激情8 全部影人</a>。<br><strong>目标</strong>：爬取速度与激情8中全部影人的图片并且用图中人物的名字给图片文件命名，最后保存在电脑中。<br><a id="more"></a><br><img src="http://upload-images.jianshu.io/upload_images/4701426-e240ffe03f1ae5d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="豆瓣1.png"></p>
<h1 id="二、运行环境"><a href="#二、运行环境" class="headerlink" title="二、运行环境"></a>二、运行环境</h1><ul>
<li>系统版本<br>Windows10  64位</li>
<li>Python版本<br>Python3.6  我用的是Anaconda集成版本</li>
<li>IDE<br>PyCharm 学生可以通过edu邮箱免费使用，不是学生的朋友可以试试社区版，不明白怎么安装的可以留言或者 私信我。</li>
</ul>
<h1 id="三、分析"><a href="#三、分析" class="headerlink" title="三、分析"></a>三、分析</h1><p>爬虫的三个要点：请求，解析，存储<br><strong>请求</strong>可以使用urllib Requests ，其中urllib是自带的，  Requests是第三方库，功能更强大，本次使用的是urllib。<br><strong>解析</strong>我用的有正则表达式，xpath，本次使用的是正则表达式，主要是想自己用正则来练练 只看正则的说明不能理解其中的奥秘ヾ(o◕∀◕)ﾉヾ，必须多试试。<br><strong>储存</strong>常用的有保存到内存，数据库，硬盘中，本次是保存到电脑硬盘中</p>
<h1 id="四、实战"><a href="#四、实战" class="headerlink" title="四、实战"></a>四、实战</h1><p>首先导入我们需要的模块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import urllib.request</div><div class="line">import os</div><div class="line">import re</div></pre></td></tr></table></figure></p>
<p>urllib.request是用来请求的，os是操作文件目录常用的模块，re是python中正则表达式的模块，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">url = &apos;https://movie.douban.com/subject/26260853/celebrities&apos;</div><div class="line">r = urllib.request.urlopen(url)</div><div class="line">html = r.read().decode(&apos;utf-8&apos;)</div></pre></td></tr></table></figure></p>
<p>第一行很明显是本次爬虫的网页， r = urllib.request.urlopen(url)用来打开网页， r.read()是读取网页内容，decode(‘utf-8’)是用utf-8编码对字符串str进行解码，以获取unicode。</p>
<p>之后我们来获取一下图片的地址，用Chrome浏览器打开速度与激情8的全部影人页面，按下F12，分析一下，可知每个人的照片地址都是img1或者3.doubanio.com/img/celebrity/medium/几个数字.jpg</p>
<p><img src="http://upload-images.jianshu.io/upload_images/4701426-a5412a6886373783.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>我们使用正则表达式来匹配一下这些图片地址，1或者3部分用\d匹配，末尾数字部分用.*来匹配即可。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">result = re.findall(r&apos;https://img\d.doubanio.com/img/celebrity/medium/.*.jpg&apos;,html)</div></pre></td></tr></table></figure></p>
<p>现在图片地址也有了，还需要把这些人物的名字给爬下来,之后才能配对文件，再次分析一下刚才的网址。看到这些人物的名字都是以title=开头，我们就用它来正则匹配一下，来获取全部的人物名字，放进一个列表中。<br><img src="http://upload-images.jianshu.io/upload_images/4701426-409edabe4ba6433c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">result2 = re.findall(r&apos;(?&lt;=title=&quot;).\S+&apos;, html)</div><div class="line">result2.pop()</div><div class="line">result3 = sorted(set(result2), key=result2.index)</div><div class="line">result3.pop(-3)</div></pre></td></tr></table></figure></p>
<p>第一行代码中re.findall(r’(?&lt;=title=”).\S+’, html)用来匹配截图中title=”后面的名字<br>第二行代码中pop()是去除最后一个元素，因为前面匹配后的列表中有一个非人物名字的元素所以我们就需要把它去掉<br>第三行代码中sorted(set(result2), key=result2.index)有两个功能，一个是使用set()集合函数来去除列表中重复元素，另一个是sorted(）函数是给列表排序用的，key=result2.index的意思是以result2原来的索引顺序来给新的列表排序，因为每张图片很名字是对应的，如果单单使用set()，虽然重复的去除了但是顺序也变了，所以我们需要利用sort()结合key=result2.index来排序才行。<br>result3.pop(-3)意思是删除result3中倒数第三个元素，因为克里斯·摩根这个没照片所以我就把他删了。</p>
<p>之后我们来给本地创建一个文件夹用来保存图片，这里就用到了os模块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">if not os.path.exists(&apos;douban&apos;):</div><div class="line">    os.makedirs(&apos;douban&apos;)</div></pre></td></tr></table></figure></p>
<p>之后需要的是下载这些人物图片，利用之前爬取的人物名字给对应图片命名并保存。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">i = 0</div><div class="line">for link in result:</div><div class="line">    filename = &apos;douban\&apos; + str(result3[i])+ &apos;.jpg&apos;</div><div class="line">    i += 1</div><div class="line">    with open(filename, &apos;w&apos;) as file:</div><div class="line">        urllib.request.urlretrieve(link, filename)</div></pre></td></tr></table></figure>
<p>完整代码贴出来，需要的同学可以试试。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">import urllib.request</div><div class="line">import os</div><div class="line">import re</div><div class="line">url = &apos;https://movie.douban.com/subject/26260853/celebrities&apos;</div><div class="line">r = urllib.request.urlopen(url)</div><div class="line">html = r.read().decode(&apos;utf-8&apos;)</div><div class="line">result = re.findall(r&apos;https://img\d.doubanio.com/img/celebrity/medium/.*.jpg&apos;, html)</div><div class="line">result2 = re.findall(r&apos;(?&lt;=title=&quot;).\S+&apos;, html)</div><div class="line">result2.pop()</div><div class="line">result3 = sorted(set(result2), key=result2.index)</div><div class="line">result3.pop(-3)</div><div class="line">if not os.path.exists(&apos;douban&apos;):</div><div class="line">    os.makedirs(&apos;douban&apos;)</div><div class="line">i = 0</div><div class="line">for link in result:</div><div class="line">    filename = &apos;douban\\&apos; + str(result3[i]) + &apos;.jpg&apos;</div><div class="line">    i += 1</div><div class="line">    with open(filename, &apos;w&apos;) as file:</div><div class="line">        urllib.request.urlretrieve(link, filename)</div></pre></td></tr></table></figure></p>
<h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h1><p>最后效果，图片都下载在我刚才指定的文件夹中了。<br><img src="http://upload-images.jianshu.io/upload_images/4701426-5c08e5da83f42dab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paste_Image.png"><br>第一次写文章，对很多东西不是很熟悉，如果有任何问题，请多多指教。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、前言&quot;&gt;&lt;a href=&quot;#一、前言&quot; class=&quot;headerlink&quot; title=&quot;一、前言&quot;&gt;&lt;/a&gt;一、前言&lt;/h1&gt;&lt;p&gt;这是我第一次写文章，作为一个非计算机，编程类专业的大二学生，我希望能够给像我这样的入门的朋友一些帮助，也同时激励自己努力写代码。好了废话不多说，今天我做的爬虫是豆瓣的一个电影——速度与激情8的全部影人页面，贴出网址：&lt;a href=&quot;https://movie.douban.com/subject/26260853/celebrities&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;速度与激情8 全部影人&lt;/a&gt;。&lt;br&gt;&lt;strong&gt;目标&lt;/strong&gt;：爬取速度与激情8中全部影人的图片并且用图中人物的名字给图片文件命名，最后保存在电脑中。&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
